{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1532bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working from /home/jason/rowan/cs02505/cs02505_4\n",
      "Loaded 675 images from ['n02099712-Labrador_retriever', 'n02107312-miniature_pinscher', 'n02100735-English_setter', 'n02113799-standard_poodle']\n",
      "Parsed 675 files\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# - Resize each cropped image to a 224 × 224 pixel image. (Similar to Assignment 1 Question 2(a))\n",
    "# - Normalize the resized image dataset.\n",
    "#\n",
    "\n",
    "# I've copied over my code from assignment 2: \n",
    "# https://github.com/kickroot/cs02505_2/blob/main/Programming%20Assignment%202.ipynb\n",
    "\n",
    "\n",
    "import os, shutil\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "print(f\"Working from {os.getcwd()}\")\n",
    "\n",
    "image_folders = [\"n02099712-Labrador_retriever\", \"n02107312-miniature_pinscher\", \n",
    "                 \"n02100735-English_setter\", \"n02113799-standard_poodle\"]\n",
    "\n",
    "\n",
    "# map breed -> list(image file names)\n",
    "image_map = dict()\n",
    "for breed in image_folders:\n",
    "    image_map[breed] = []\n",
    "\n",
    "image_count = 0\n",
    "for folder in image_folders:    \n",
    "    for (root,dirs,files) in os.walk(\"images/\" + folder):    \n",
    "        head, tail = os.path.split(root)\n",
    "        for file in files:\n",
    "            image_count += 1\n",
    "            image_map[tail].append(file)        \n",
    "print(f\"Loaded {image_count} images from {image_folders}\")\n",
    "\n",
    "#\n",
    "# Build up and apply the bounding boxes to all images\n",
    "#\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# For a given file path, return a list of BoundingBox objects. A single file may have more than one\n",
    "# bounding box!\n",
    "\n",
    "# The following function was inspired by https://www.kaggle.com/code/espriella/stanford-dogs-transfer-crop-stack/notebook\n",
    "def get_boxes(file_path):\n",
    "    boxes = []\n",
    "    \n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    bbox = []\n",
    "    for o in objects:\n",
    "        bndbox = o.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)        \n",
    "        boxes.append({\"xmin\": xmin, \"ymin\" : ymin, \"xmax\": xmax, \"ymax\": ymax})\n",
    "    \n",
    "    return boxes\n",
    "    \n",
    "    \n",
    "# Map <image file name> -> <bounding boxes>\n",
    "bounding_boxes = dict()\n",
    "\n",
    "#\n",
    "# Loop through all annotations and map bounding boxes to image names.  Since the names appear globally unique \n",
    "# we can make use of a global index using only file names (no breed paths required)\n",
    "#\n",
    "for (root,dirs,files) in os.walk(\"annotations\"):    \n",
    "    for anno in files:        \n",
    "        bounding_boxes[anno] = get_boxes(f\"{root}/{anno}\")\n",
    "\n",
    "        \n",
    "print(f\"Parsed {len(bounding_boxes)} files\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#\n",
    "# For each image, apply the bounding box and write a newly cropped+scaled image into the\n",
    "# cropped/ folder.\n",
    "#\n",
    "\n",
    "def crop_resize_write(cropped_folder, breed, image_file, bounding_box):\n",
    "    # Bounding box my contain more than one box, for simplicity we're only going to use the first\n",
    "    # one as nothing in the instructions listed supporting multiple as a requirement.\n",
    "    bb = bounding_box[0]\n",
    "    head, tail = os.path.split(breed)\n",
    "    im = Image.open(f\"{breed}/{image_file}\")\n",
    "    im = im.crop((bb['xmin'], bb['ymin'], bb['xmax'], bb['ymax'])).resize((224, 224), Image.LANCZOS)\n",
    "    im = im.convert('RGB')\n",
    "    im.save(f\"{cropped_folder}/{tail}/{image_file}\")\n",
    "\n",
    "# Let's start with a fresh folder structure\n",
    "cropped_folder = \"cropped\"\n",
    "if os.path.isdir(cropped_folder):\n",
    "    shutil.rmtree(cropped_folder)    \n",
    "    \n",
    "    \n",
    "for folder in image_folders:\n",
    "  os.makedirs(f\"{cropped_folder}/{folder}\")  \n",
    "\n",
    "# Iterate over the image files and apply the bounding box\n",
    "for (root,dirs,files) in os.walk(\"images\"):    \n",
    "    for image_file in files:\n",
    "        key = image_file.replace(\".jpg\", \"\")\n",
    "        bb = bounding_boxes[key]\n",
    "        crop_resize_write(cropped_folder, root, image_file, bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e9b409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Normalize the resized image dataset.\n",
    "#\n",
    "normalized_images = dict()\n",
    "for breed in image_folders:\n",
    "    normalized_images[breed] = []\n",
    "\n",
    "# normalized_folder = \"normalized\"\n",
    "# if os.path.isdir(normalized_folder):\n",
    "#     shutil.rmtree(normalized_folder)    \n",
    "    \n",
    "# for folder in image_folders:\n",
    "#   os.makedirs(f\"{normalized_folder}/{folder}\")\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Iterate over the image files and apply normalization as described in\n",
    "# https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/\n",
    "for (root,dirs,files) in os.walk(cropped_folder):    \n",
    "    for image_file in files:\n",
    "        img = Image.open(f\"{root}/{image_file}\")\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        img_tr = transform(img)\n",
    "        mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
    "        \n",
    "        transform_norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "        img_normalized = transform_norm(img)\n",
    "        normalized_images[os.path.split(root)[1]].append(img_normalized)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd4ba2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Extract features for each image from the last convolution layer of “ResNet18” (You can follow\n",
    "# https://kozodoi.me/blog/20210527/extracting-features. But you must reference\n",
    "# this website in your solution) (2.5 points)\n",
    "#\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = timm.create_model(model_name = 'resnet18', pretrained = True)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.global_pool.register_forward_hook(get_features('feats'))\n",
    "\n",
    "# placeholders\n",
    "PREDS = []\n",
    "FEATS = []\n",
    "Y = []\n",
    "\n",
    "# placeholder for batch features\n",
    "features = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00bee1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, tensor_list in normalized_images.items():\n",
    "    for t in tensor_list:\n",
    "        t.to(device)\n",
    "        preds = model(t.unsqueeze(0))\n",
    "        Y.append(label)\n",
    "        PREDS.append(preds.detach().cpu().numpy())\n",
    "        FEATS.append(features['feats'].cpu().numpy())    \n",
    "    \n",
    "import numpy as np\n",
    "PREDS = np.concatenate(PREDS)\n",
    "FEATS = np.concatenate(FEATS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61afefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2. (Dimension Reduction) Perform dimension reduction on your new dog image representation dataset\n",
    "# to reduce the dimension to 2 (similar to Assignment 1 Question 2(f)). (0.5 points)\n",
    "#\n",
    "\n",
    "# This is based on my original code here: https://github.com/kickroot/cs02505_1/blob/main/assignment_1.ipynb\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(FEATS)\n",
    "X=pca.transform(FEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "def40549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 3. (Clustering Algorithm) Perform clustering using the following approaches on the 2D dataset you\n",
    "# preprocessed in Item 2:\n",
    "#\n",
    "\n",
    "results = dict()\n",
    "\n",
    "#\n",
    "# (a) K-means clustering: (Use KMeans with init = ‘Random’) (0.5 point)\n",
    "#\n",
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(init=\"random\", n_clusters=4, n_init=10)\n",
    "results[\"k_means\"] = k_means.fit_transform(X)\n",
    "\n",
    "\n",
    "#\n",
    "# (b) KMeans with init=‘k-means++’ (0.5 point)\n",
    "#\n",
    "k_means = KMeans(init=\"k-means++\", n_clusters=4, n_init=10)\n",
    "results[\"k_means++\"] = k_means.fit_transform(X)\n",
    "\n",
    "#\n",
    "# (c) Bisecting K-means (sklearn.cluster.BisectingKMeans with init = ‘Random’) (0.5 point)\n",
    "#\n",
    "from sklearn.cluster import BisectingKMeans\n",
    "k_means = KMeans(init=\"random\", n_clusters=4, n_init=10)\n",
    "results[\"bisecting_k_means\"] = k_means.fit_transform(X)\n",
    "\n",
    "#\n",
    "# (d) spectral clustering (sklearn.cluster.SpectralClustering with default parameters) (0.5 point)\n",
    "#\n",
    "from sklearn.cluster import SpectralClustering\n",
    "k_means = SpectralClustering(n_clusters=4)\n",
    "results[\"spectral\"] = k_means.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DBSCAN (0.5 point)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
